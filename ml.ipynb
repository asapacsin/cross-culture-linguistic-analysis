{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdfa750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from file_process import load_corpus\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "from openpyxl import load_workbook\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f8a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file\n",
    "chinese_folder = \"../data/chinese/\"\n",
    "english_folder = \"../data/english/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8c42d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing genre: history_fiction\n",
      "  Success: 1. 长安十二时辰 (马伯庸) (Z-Library).txt                    → 7,570 words\n",
      "  Success: 10. 历史的裂变：中国历史上的十三场政变（畅销书《大唐兴亡三百年》作者王觉仁力作，用小说笔法，讲述 → 4,591 words\n",
      "  Success: 2. 风起陇西 (马伯庸 [马伯庸]) (Z-Library).txt                → 4,151 words\n",
      "  Success: 3. 隋乱 (酒徒) (Z-Library).txt                         → 24,246 words\n",
      "  Success: 4. 新宋 (阿越) (Z-Library).txt                         → 41,427 words\n",
      "  Success: 5. 步步生莲 (月关) (Z-Library).txt                       → 45,475 words\n",
      "  Success: 6. 宰执天下 (cuslaa) (Z-Library).txt                   → 138,306 words\n",
      "  Success: 7. 窃明 (灰熊猫) (Z-Library).txt                        → 18,725 words\n",
      "  Success: 8. 四时歌：骑桶人自选集 (骑桶人) (Z-Library).txt                → 2,211 words\n",
      "  Success: 9. 辛亥：计划外革命 (雪珥) (Z-Library).txt                   → 1,649 words\n",
      "\n",
      "Processing genre: horror\n",
      "  Success: 1. 精绝古城 (天下霸唱) (Z-Library).txt                     → 2,762 words\n",
      "  Success: 10. 死亡通知单系列（套装5本）（死亡通知单+死亡通知单之离别曲（上）+死亡通知单之离别曲（下）+ → 17,152 words\n",
      "  Success: 2. 鬼吹灯之龙岭迷窟 (天下霸唱) (Z-Library).txt                 → 2,001 words\n",
      "  Success: 3. 荒村公寓 (蔡骏) (Z-Library).txt                       → 3,649 words\n",
      "  Success: 4. 《地狱的第十九层》 (蔡骏) (Z-Library).txt                  → 2,077 words\n",
      "  Success: 5. 奇门遁甲 (周德东) (Z-Library).txt                      → 5,161 words\n",
      "Failed to decode (tried all encodings): ../data/chinese/horror\\6. 血钞票.txt (血钞票.txt) (Z-Library).txt\n",
      "  Failed reading: 6. 血钞票.txt (血钞票.txt) (Z-Library).txt\n",
      "  Success: 7. 周德东恐怖悬疑小说合集 (周德东) (Z-Library).txt               → 183,610 words\n",
      "  Success: 8. 必须犯规的游戏 (宁航一 [宁航一]) (Z-Library).txt             → 18,285 words\n",
      "  Success: 9. 天坑宝藏 (（电子书优先上架）2020年天下霸唱开年新作 明星影响力IP“天坑”系列 马殿臣、 → 668 words\n",
      "\n",
      "Processing genre: school_ya\n",
      "  Success: 1. 最好的我们 (八月长安 [八月长安]) (Z-Library).txt             → 7,468 words\n",
      "  Success: 10. 暗恋·橘生淮南 (八月长安 [八月长安]) (Z-Library).txt          → 9,703 words\n",
      "  Success: 2. 你好，旧时光（三周年完美纪念版）（套装共3册） (八月长安 [八月长安]) (Z-Librar → 12,012 words\n",
      "  Success: 3. 致我们单纯的小美好 (赵乾乾) (Z-Library).txt                 → 4,495 words\n",
      "  Success: 4. 不如去闯（与其迷茫，不如去闯！耶鲁哈佛毕业、90后总裁、学长李柘远LEO送给年轻人的干货+动力 → 4,339 words\n",
      "  Success: 5. 云边有个小卖部（张嘉佳全新力作！写给我们所遇见的悲伤和希望，和路上从未断绝的一缕光！） (张嘉 → 3,263 words\n",
      "  Success: 6. 少年的你，如此美丽 (玖月晞 [jiuyuexi]) (Z-Library).txt      → 11,779 words\n",
      "  Success: 7. 左耳 (饶雪漫) (Z-Library).txt                        → 2,948 words\n",
      "  Success: 8. 玖月晞亲爱的系列（套装2册） (玖月晞 [jiuyuexi]) (Z-Library).txt → 28,006 words\n",
      "  Success: 9. 时间的女儿 (八月长安) (Z-Library).txt                    → 1,874 words\n",
      "\n",
      "Processing genre: scifi\n",
      "  Success: 1. 劉慈欣《三體》.txt                                     → 2,615 words\n",
      "  Success: 10.-与机器人同行——阿缺中短篇科幻小说选-_中国科幻基石丛书_-_阿缺_-_Z-Library_ → 4,693 words\n",
      "  Success: 2. 郝景芳-北京折叠.txt                                    → 298 words\n",
      "  Success: 3.-劉慈欣《三體Ⅱ：黑暗森林》_1.txt                             → 5,001 words\n",
      "  Success: 5.-银河之心三部曲（天垂日暮-暗黑深渊-逐影追光）中国科幻银河奖桂冠作家史诗巨著-_中国科幻基石丛 → 18,647 words\n",
      "  Success: 6.-伪人2075·意识重组-_迟卉_-_Z-Library_.txt                → 3,788 words\n",
      "  Success: 7.-宇宙涟漪中的孩子-_谢云宁_-_Z-Library_.txt                  → 2,757 words\n",
      "  Success: 8. 决战奇点（华语科幻星云奖、首届晨星奖、晋康奖获得者萧星寒，全新科幻力作！中国科幻四大天王联袂推 → 4,697 words\n",
      "  Success: 9.-小镇奇谈（充满想象力和少年感的极限科幻之作！银河奖得主，《群星》作者七月最新作品，陈思诚、潘海 → 4,448 words\n",
      "  Success: 9.-群星-_七月_-_Z-Library_.txt                         → 3,082 words\n",
      "\n",
      "Finished! Successfully loaded 39/40 files.\n",
      "\n",
      "Processing genre: history_fiction\n",
      "  Success: All the Light We Cannot See - Anthony Doerr.txt    → 129,105 words\n",
      "  Success: Bring Up the Bodies - Hilary Mantel.txt            → 141,952 words\n",
      "  Success: Fall of Giants - Ken Follett.txt                   → 319,676 words\n",
      "  Success: March - Geraldine Brooks.txt                       → 98,789 words\n",
      "  Success: 10. 死亡通知单系列（套装5本）（死亡通知单+死亡通知单之离别曲（上）+死亡通知单之离别曲（下）+ → 17,152 words\n",
      "  Success: 2. 鬼吹灯之龙岭迷窟 (天下霸唱) (Z-Library).txt                 → 2,001 words\n",
      "  Success: 3. 荒村公寓 (蔡骏) (Z-Library).txt                       → 3,649 words\n",
      "  Success: 4. 《地狱的第十九层》 (蔡骏) (Z-Library).txt                  → 2,077 words\n",
      "  Success: 5. 奇门遁甲 (周德东) (Z-Library).txt                      → 5,161 words\n",
      "Failed to decode (tried all encodings): ../data/chinese/horror\\6. 血钞票.txt (血钞票.txt) (Z-Library).txt\n",
      "  Failed reading: 6. 血钞票.txt (血钞票.txt) (Z-Library).txt\n",
      "  Success: 7. 周德东恐怖悬疑小说合集 (周德东) (Z-Library).txt               → 183,610 words\n",
      "  Success: 8. 必须犯规的游戏 (宁航一 [宁航一]) (Z-Library).txt             → 18,285 words\n",
      "  Success: 9. 天坑宝藏 (（电子书优先上架）2020年天下霸唱开年新作 明星影响力IP“天坑”系列 马殿臣、 → 668 words\n",
      "\n",
      "Processing genre: school_ya\n",
      "  Success: 1. 最好的我们 (八月长安 [八月长安]) (Z-Library).txt             → 7,468 words\n",
      "  Success: 10. 暗恋·橘生淮南 (八月长安 [八月长安]) (Z-Library).txt          → 9,703 words\n",
      "  Success: 2. 你好，旧时光（三周年完美纪念版）（套装共3册） (八月长安 [八月长安]) (Z-Librar → 12,012 words\n",
      "  Success: 3. 致我们单纯的小美好 (赵乾乾) (Z-Library).txt                 → 4,495 words\n",
      "  Success: 4. 不如去闯（与其迷茫，不如去闯！耶鲁哈佛毕业、90后总裁、学长李柘远LEO送给年轻人的干货+动力 → 4,339 words\n",
      "  Success: 5. 云边有个小卖部（张嘉佳全新力作！写给我们所遇见的悲伤和希望，和路上从未断绝的一缕光！） (张嘉 → 3,263 words\n",
      "  Success: 6. 少年的你，如此美丽 (玖月晞 [jiuyuexi]) (Z-Library).txt      → 11,779 words\n",
      "  Success: 7. 左耳 (饶雪漫) (Z-Library).txt                        → 2,948 words\n",
      "  Success: 8. 玖月晞亲爱的系列（套装2册） (玖月晞 [jiuyuexi]) (Z-Library).txt → 28,006 words\n",
      "  Success: 9. 时间的女儿 (八月长安) (Z-Library).txt                    → 1,874 words\n",
      "\n",
      "Processing genre: scifi\n",
      "  Success: 1. 劉慈欣《三體》.txt                                     → 2,615 words\n",
      "  Success: 10.-与机器人同行——阿缺中短篇科幻小说选-_中国科幻基石丛书_-_阿缺_-_Z-Library_ → 4,693 words\n",
      "  Success: 2. 郝景芳-北京折叠.txt                                    → 298 words\n",
      "  Success: 3.-劉慈欣《三體Ⅱ：黑暗森林》_1.txt                             → 5,001 words\n",
      "  Success: 5.-银河之心三部曲（天垂日暮-暗黑深渊-逐影追光）中国科幻银河奖桂冠作家史诗巨著-_中国科幻基石丛 → 18,647 words\n",
      "  Success: 6.-伪人2075·意识重组-_迟卉_-_Z-Library_.txt                → 3,788 words\n",
      "  Success: 7.-宇宙涟漪中的孩子-_谢云宁_-_Z-Library_.txt                  → 2,757 words\n",
      "  Success: 8. 决战奇点（华语科幻星云奖、首届晨星奖、晋康奖获得者萧星寒，全新科幻力作！中国科幻四大天王联袂推 → 4,697 words\n",
      "  Success: 9.-小镇奇谈（充满想象力和少年感的极限科幻之作！银河奖得主，《群星》作者七月最新作品，陈思诚、潘海 → 4,448 words\n",
      "  Success: 9.-群星-_七月_-_Z-Library_.txt                         → 3,082 words\n",
      "\n",
      "Finished! Successfully loaded 39/40 files.\n",
      "\n",
      "Processing genre: history_fiction\n",
      "  Success: All the Light We Cannot See - Anthony Doerr.txt    → 129,105 words\n",
      "  Success: Bring Up the Bodies - Hilary Mantel.txt            → 141,952 words\n",
      "  Success: Fall of Giants - Ken Follett.txt                   → 319,676 words\n",
      "  Success: March - Geraldine Brooks.txt                       → 98,789 words\n",
      "  Success: 10. 死亡通知单系列（套装5本）（死亡通知单+死亡通知单之离别曲（上）+死亡通知单之离别曲（下）+ → 17,152 words\n",
      "  Success: 2. 鬼吹灯之龙岭迷窟 (天下霸唱) (Z-Library).txt                 → 2,001 words\n",
      "  Success: 3. 荒村公寓 (蔡骏) (Z-Library).txt                       → 3,649 words\n",
      "  Success: 4. 《地狱的第十九层》 (蔡骏) (Z-Library).txt                  → 2,077 words\n",
      "  Success: 5. 奇门遁甲 (周德东) (Z-Library).txt                      → 5,161 words\n",
      "Failed to decode (tried all encodings): ../data/chinese/horror\\6. 血钞票.txt (血钞票.txt) (Z-Library).txt\n",
      "  Failed reading: 6. 血钞票.txt (血钞票.txt) (Z-Library).txt\n",
      "  Success: 7. 周德东恐怖悬疑小说合集 (周德东) (Z-Library).txt               → 183,610 words\n",
      "  Success: 8. 必须犯规的游戏 (宁航一 [宁航一]) (Z-Library).txt             → 18,285 words\n",
      "  Success: 9. 天坑宝藏 (（电子书优先上架）2020年天下霸唱开年新作 明星影响力IP“天坑”系列 马殿臣、 → 668 words\n",
      "\n",
      "Processing genre: school_ya\n",
      "  Success: 1. 最好的我们 (八月长安 [八月长安]) (Z-Library).txt             → 7,468 words\n",
      "  Success: 10. 暗恋·橘生淮南 (八月长安 [八月长安]) (Z-Library).txt          → 9,703 words\n",
      "  Success: 2. 你好，旧时光（三周年完美纪念版）（套装共3册） (八月长安 [八月长安]) (Z-Librar → 12,012 words\n",
      "  Success: 3. 致我们单纯的小美好 (赵乾乾) (Z-Library).txt                 → 4,495 words\n",
      "  Success: 4. 不如去闯（与其迷茫，不如去闯！耶鲁哈佛毕业、90后总裁、学长李柘远LEO送给年轻人的干货+动力 → 4,339 words\n",
      "  Success: 5. 云边有个小卖部（张嘉佳全新力作！写给我们所遇见的悲伤和希望，和路上从未断绝的一缕光！） (张嘉 → 3,263 words\n",
      "  Success: 6. 少年的你，如此美丽 (玖月晞 [jiuyuexi]) (Z-Library).txt      → 11,779 words\n",
      "  Success: 7. 左耳 (饶雪漫) (Z-Library).txt                        → 2,948 words\n",
      "  Success: 8. 玖月晞亲爱的系列（套装2册） (玖月晞 [jiuyuexi]) (Z-Library).txt → 28,006 words\n",
      "  Success: 9. 时间的女儿 (八月长安) (Z-Library).txt                    → 1,874 words\n",
      "\n",
      "Processing genre: scifi\n",
      "  Success: 1. 劉慈欣《三體》.txt                                     → 2,615 words\n",
      "  Success: 10.-与机器人同行——阿缺中短篇科幻小说选-_中国科幻基石丛书_-_阿缺_-_Z-Library_ → 4,693 words\n",
      "  Success: 2. 郝景芳-北京折叠.txt                                    → 298 words\n",
      "  Success: 3.-劉慈欣《三體Ⅱ：黑暗森林》_1.txt                             → 5,001 words\n",
      "  Success: 5.-银河之心三部曲（天垂日暮-暗黑深渊-逐影追光）中国科幻银河奖桂冠作家史诗巨著-_中国科幻基石丛 → 18,647 words\n",
      "  Success: 6.-伪人2075·意识重组-_迟卉_-_Z-Library_.txt                → 3,788 words\n",
      "  Success: 7.-宇宙涟漪中的孩子-_谢云宁_-_Z-Library_.txt                  → 2,757 words\n",
      "  Success: 8. 决战奇点（华语科幻星云奖、首届晨星奖、晋康奖获得者萧星寒，全新科幻力作！中国科幻四大天王联袂推 → 4,697 words\n",
      "  Success: 9.-小镇奇谈（充满想象力和少年感的极限科幻之作！银河奖得主，《群星》作者七月最新作品，陈思诚、潘海 → 4,448 words\n",
      "  Success: 9.-群星-_七月_-_Z-Library_.txt                         → 3,082 words\n",
      "\n",
      "Finished! Successfully loaded 39/40 files.\n",
      "\n",
      "Processing genre: history_fiction\n",
      "  Success: All the Light We Cannot See - Anthony Doerr.txt    → 129,105 words\n",
      "  Success: Bring Up the Bodies - Hilary Mantel.txt            → 141,952 words\n",
      "  Success: Fall of Giants - Ken Follett.txt                   → 319,676 words\n",
      "  Success: March - Geraldine Brooks.txt                       → 98,789 words\n",
      "  Success: Paradise Alley - Kevin Baker.txt                   → 256,844 words\n",
      "  Success: The Four Winds - Kristin Hannah.txt                → 126,473 words\n",
      "  Success: The Last Kingdom - Bernard Cornwell.txt            → 122,106 words\n",
      "  Success: The Nightingale - Kristin Hannah.txt               → 148,972 words\n",
      "  Success: The Song of Achilles - Madeline Miller.txt         → 101,275 words\n",
      "  Success: Wolf Hall - Hilary Mantel.txt                      → 219,123 words\n",
      "\n",
      "Processing genre: horror\n",
      "  Success: A Head Full of Ghosts - Paul Tremblay.txt          → 89,120 words\n",
      "  Success: Bird Box - Malerman, Josh.txt                      → 69,598 words\n",
      "  Success: Come With Me - Ronald Malfi.txt                    → 114,271 words\n",
      "  Success: Doctor Sleep - Stephen King.txt                    → 164,394 words\n",
      "  Success: Duma Key - Stephen King.txt                        → 204,052 words\n",
      "  Success: Heart-Shaped Box - Joe Hill.txt                    → 105,327 words\n",
      "  Success: Mexican Gothic - Silvia Moreno-Garcia.txt          → 93,415 words\n",
      "  Success: Seed - Ania Ahlborn.txt                            → 70,519 words\n",
      "  Success: The Final Girl Support Group - Grady Hendrix.txt   → 92,106 words\n",
      "  Success: The Last House on Needless Street - Catriona Ward. → 97,425 words\n",
      "\n",
      "Processing genre: school_ya\n",
      "  Success: All the Bright Places - Jennifer Niven.txt         → 94,193 words\n",
      "  Success: Divergent - Veronica Roth.txt                      → 108,503 words\n",
      "  Success: Eleanor & Park - Rainbow Rowell.txt                → 80,185 words\n",
      "  Success: P.S. I Like You - Kasie West.txt                   → 70,624 words\n",
      "  Success: The Cousins - Karen McManus.txt                    → 87,807 words\n",
      "  Success: The Fill-In Boyfriend - Kasie West.txt             → 66,853 words\n",
      "  Success: The Hate U Give - Angie Thomas.txt                 → 96,996 words\n",
      "  Success: The Sun Is Also a Star - Nicola Yoon.txt           → 69,459 words\n",
      "  Success: To All the Boys I've Loved Before - Jenny Han.txt  → 82,519 words\n",
      "  Success: Twilight - Stephenie Meyer.txt                     → 119,914 words\n",
      "\n",
      "Processing genre: scifi\n",
      "  Success: All Systems Red - Martha Wells.txt                 → 31,606 words\n",
      "  Success: Paradise Alley - Kevin Baker.txt                   → 256,844 words\n",
      "  Success: The Four Winds - Kristin Hannah.txt                → 126,473 words\n",
      "  Success: The Last Kingdom - Bernard Cornwell.txt            → 122,106 words\n",
      "  Success: The Nightingale - Kristin Hannah.txt               → 148,972 words\n",
      "  Success: The Song of Achilles - Madeline Miller.txt         → 101,275 words\n",
      "  Success: Wolf Hall - Hilary Mantel.txt                      → 219,123 words\n",
      "\n",
      "Processing genre: horror\n",
      "  Success: A Head Full of Ghosts - Paul Tremblay.txt          → 89,120 words\n",
      "  Success: Bird Box - Malerman, Josh.txt                      → 69,598 words\n",
      "  Success: Come With Me - Ronald Malfi.txt                    → 114,271 words\n",
      "  Success: Doctor Sleep - Stephen King.txt                    → 164,394 words\n",
      "  Success: Duma Key - Stephen King.txt                        → 204,052 words\n",
      "  Success: Heart-Shaped Box - Joe Hill.txt                    → 105,327 words\n",
      "  Success: Mexican Gothic - Silvia Moreno-Garcia.txt          → 93,415 words\n",
      "  Success: Seed - Ania Ahlborn.txt                            → 70,519 words\n",
      "  Success: The Final Girl Support Group - Grady Hendrix.txt   → 92,106 words\n",
      "  Success: The Last House on Needless Street - Catriona Ward. → 97,425 words\n",
      "\n",
      "Processing genre: school_ya\n",
      "  Success: All the Bright Places - Jennifer Niven.txt         → 94,193 words\n",
      "  Success: Divergent - Veronica Roth.txt                      → 108,503 words\n",
      "  Success: Eleanor & Park - Rainbow Rowell.txt                → 80,185 words\n",
      "  Success: P.S. I Like You - Kasie West.txt                   → 70,624 words\n",
      "  Success: The Cousins - Karen McManus.txt                    → 87,807 words\n",
      "  Success: The Fill-In Boyfriend - Kasie West.txt             → 66,853 words\n",
      "  Success: The Hate U Give - Angie Thomas.txt                 → 96,996 words\n",
      "  Success: The Sun Is Also a Star - Nicola Yoon.txt           → 69,459 words\n",
      "  Success: To All the Boys I've Loved Before - Jenny Han.txt  → 82,519 words\n",
      "  Success: Twilight - Stephenie Meyer.txt                     → 119,914 words\n",
      "\n",
      "Processing genre: scifi\n",
      "  Success: All Systems Red - Martha Wells.txt                 → 31,606 words\n",
      "  Success: Paradise Alley - Kevin Baker.txt                   → 256,844 words\n",
      "  Success: The Four Winds - Kristin Hannah.txt                → 126,473 words\n",
      "  Success: The Last Kingdom - Bernard Cornwell.txt            → 122,106 words\n",
      "  Success: The Nightingale - Kristin Hannah.txt               → 148,972 words\n",
      "  Success: The Song of Achilles - Madeline Miller.txt         → 101,275 words\n",
      "  Success: Wolf Hall - Hilary Mantel.txt                      → 219,123 words\n",
      "\n",
      "Processing genre: horror\n",
      "  Success: A Head Full of Ghosts - Paul Tremblay.txt          → 89,120 words\n",
      "  Success: Bird Box - Malerman, Josh.txt                      → 69,598 words\n",
      "  Success: Come With Me - Ronald Malfi.txt                    → 114,271 words\n",
      "  Success: Doctor Sleep - Stephen King.txt                    → 164,394 words\n",
      "  Success: Duma Key - Stephen King.txt                        → 204,052 words\n",
      "  Success: Heart-Shaped Box - Joe Hill.txt                    → 105,327 words\n",
      "  Success: Mexican Gothic - Silvia Moreno-Garcia.txt          → 93,415 words\n",
      "  Success: Seed - Ania Ahlborn.txt                            → 70,519 words\n",
      "  Success: The Final Girl Support Group - Grady Hendrix.txt   → 92,106 words\n",
      "  Success: The Last House on Needless Street - Catriona Ward. → 97,425 words\n",
      "\n",
      "Processing genre: school_ya\n",
      "  Success: All the Bright Places - Jennifer Niven.txt         → 94,193 words\n",
      "  Success: Divergent - Veronica Roth.txt                      → 108,503 words\n",
      "  Success: Eleanor & Park - Rainbow Rowell.txt                → 80,185 words\n",
      "  Success: P.S. I Like You - Kasie West.txt                   → 70,624 words\n",
      "  Success: The Cousins - Karen McManus.txt                    → 87,807 words\n",
      "  Success: The Fill-In Boyfriend - Kasie West.txt             → 66,853 words\n",
      "  Success: The Hate U Give - Angie Thomas.txt                 → 96,996 words\n",
      "  Success: The Sun Is Also a Star - Nicola Yoon.txt           → 69,459 words\n",
      "  Success: To All the Boys I've Loved Before - Jenny Han.txt  → 82,519 words\n",
      "  Success: Twilight - Stephenie Meyer.txt                     → 119,914 words\n",
      "\n",
      "Processing genre: scifi\n",
      "  Success: All Systems Red - Martha Wells.txt                 → 31,606 words\n",
      "  Success: Ancillary Justice - Ann Leckie.txt                 → 113,925 words\n",
      "  Success: Old Man's War - John Scalzi.txt                    → 91,526 words\n",
      "  Success: Ready Player One - Ernest Cline.txt                → 137,752 words\n",
      "  Success: The Fifth Season - N. K. Jemisin.txt               → 136,614 words\n",
      "  Success: The Hunger Games - Suzanne Collins.txt             → 100,229 words\n",
      "  Success: The Long Way to a Small, Angry Planet - Becky Cham → 131,662 words\n",
      "  Success: The Martian - Andy Weir.txt                        → 100,790 words\n",
      "  Success: Uprooted - Naomi Novik.txt                         → 156,219 words\n",
      "  Success: We Are Legion (We Are Bob) - Dennis E. Taylor.txt  → 92,073 words\n",
      "\n",
      "Finished! Successfully loaded 40/40 files.\n",
      "  Success: Ancillary Justice - Ann Leckie.txt                 → 113,925 words\n",
      "  Success: Old Man's War - John Scalzi.txt                    → 91,526 words\n",
      "  Success: Ready Player One - Ernest Cline.txt                → 137,752 words\n",
      "  Success: The Fifth Season - N. K. Jemisin.txt               → 136,614 words\n",
      "  Success: The Hunger Games - Suzanne Collins.txt             → 100,229 words\n",
      "  Success: The Long Way to a Small, Angry Planet - Becky Cham → 131,662 words\n",
      "  Success: The Martian - Andy Weir.txt                        → 100,790 words\n",
      "  Success: Uprooted - Naomi Novik.txt                         → 156,219 words\n",
      "  Success: We Are Legion (We Are Bob) - Dennis E. Taylor.txt  → 92,073 words\n",
      "\n",
      "Finished! Successfully loaded 40/40 files.\n",
      "  Success: Ancillary Justice - Ann Leckie.txt                 → 113,925 words\n",
      "  Success: Old Man's War - John Scalzi.txt                    → 91,526 words\n",
      "  Success: Ready Player One - Ernest Cline.txt                → 137,752 words\n",
      "  Success: The Fifth Season - N. K. Jemisin.txt               → 136,614 words\n",
      "  Success: The Hunger Games - Suzanne Collins.txt             → 100,229 words\n",
      "  Success: The Long Way to a Small, Angry Planet - Becky Cham → 131,662 words\n",
      "  Success: The Martian - Andy Weir.txt                        → 100,790 words\n",
      "  Success: Uprooted - Naomi Novik.txt                         → 156,219 words\n",
      "  Success: We Are Legion (We Are Bob) - Dennis E. Taylor.txt  → 92,073 words\n",
      "\n",
      "Finished! Successfully loaded 40/40 files.\n"
     ]
    }
   ],
   "source": [
    "chinese_df = load_corpus(chinese_folder,lang=\"chinese\")\n",
    "english_df = load_corpus(english_folder,lang=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d80b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = pd.read_excel('english_df.xlsx')\n",
    "chinese_df = pd.read_excel('chinese_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c20f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = english_df.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "780bcc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub\\models--FacebookAI--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"FacebookAI/xlm-roberta-base\",   # or -large\n",
    "    num_labels=len(genres)   # ← this is the only thing you set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78e7acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare English-only samples and labels\n",
    "if \"language\" in english_df.columns:\n",
    "    english_texts = english_df[english_df[\"language\"].str.lower().str.startswith(\"en\")].reset_index(drop=True)\n",
    "english_texts = english_texts.dropna(subset=[\"genre\"]).reset_index(drop=True)\n",
    "if len(english_texts) == 0:\n",
    "    raise ValueError(\"No English samples found for classification.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20753563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create label dictionaries\n",
    "label2id = {g: i for i, g in enumerate(sorted(english_texts[\"genre\"].unique()))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7ec14ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset for genre classification reading text directly from DataFrame\n",
    "class GenreDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, label2id, max_length=256):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row.get(\"text\", \"\") or \"\"\n",
    "# pull raw text from DataFrame\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        text = text.strip()\n",
    "        if not text:\n",
    "            text = \" \"\n",
    "# avoid empty strings for tokenizer\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoded = {k: v.squeeze(0) for k, v in encoded.items()}\n",
    "        encoded[\"labels\"] = torch.tensor(self.label2id[row[\"genre\"]], dtype=torch.long)\n",
    "        return encoded\n",
    "\n",
    "# Instantiate dataset and collator\n",
    "dataset = GenreDataset(english_texts, tokenizer, label2id, max_length=256)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba39931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add optimizer (e.g., Adam, SGD, AdamW, etc.)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a837f35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45125c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80/20 train/validation split\n",
    "total_samples = len(dataset)\n",
    "if total_samples < 2:\n",
    "    raise ValueError(\"Need at least 2 English samples for an 80/20 split.\")\n",
    "train_size = max(1, int(0.8 * total_samples))\n",
    "val_size = total_samples - train_size\n",
    "if val_size == 0:\n",
    "    val_size = 1\n",
    "    train_size = total_samples - 1\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=generator)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, collate_fn=data_collator)\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f54905d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=1.4273 val_loss=1.5567 val_acc=0.000 best_val_loss=1.5567\n",
      "Epoch 2: train_loss=1.3801 val_loss=1.5472 val_acc=0.000 best_val_loss=1.5472\n",
      "Epoch 2: train_loss=1.3801 val_loss=1.5472 val_acc=0.000 best_val_loss=1.5472\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     15\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     17\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[35], line 22\u001b[0m, in \u001b[0;36mGenreDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     20\u001b[0m             text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# avoid empty strings for tokenizer\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m         encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m         encoded \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m encoded\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     30\u001b[0m         encoded[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel2id[row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3073\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   3072\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 3073\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   3074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3075\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3183\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   3162\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3163\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3180\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3181\u001b[0m     )\n\u001b[0;32m   3182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   3184\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3185\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   3186\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3187\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3188\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   3189\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3190\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3191\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3192\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3193\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   3194\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3195\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3196\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3197\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3198\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3199\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3200\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3201\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3202\u001b[0m         split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   3203\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3204\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3258\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3229\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3230\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[0;32m   3231\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[0;32m   3247\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3249\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3250\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3251\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3255\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3256\u001b[0m )\n\u001b[1;32m-> 3258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   3259\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3260\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   3261\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3262\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3263\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3264\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3265\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3266\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3267\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3268\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   3269\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3270\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3271\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3272\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3273\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3274\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3275\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3276\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3277\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_special_tokens),\n\u001b[0;32m   3278\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3279\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\transformers\\tokenization_utils.py:800\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    798\u001b[0m     )\n\u001b[1;32m--> 800\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    804\u001b[0m     first_ids,\n\u001b[0;32m    805\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    821\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\transformers\\tokenization_utils.py:767\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 767\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    768\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\transformers\\tokenization_utils.py:697\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 697\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    698\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:161\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[1;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[0;32m    159\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[0;32m    166\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:310\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# union() returns a new set by concatenating the two sets.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m never_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(never_split)) \u001b[38;5;28;01mif\u001b[39;00m never_split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\n\u001b[1;32m--> 310\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:412\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m    411\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFFD\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_is_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_whitespace(char):\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\lingu\\lib\\site-packages\\transformers\\tokenization_utils.py:361\u001b[0m, in \u001b[0;36m_is_control\u001b[1;34m(char)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    360\u001b[0m cat \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mcategory(char)\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cat\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Simple training + validation loop with scheduler and grad clipping\n",
    "num_epochs = 5\n",
    "total_steps = max(1, num_epochs * len(train_loader))\n",
    "warmup_steps = max(1, int(0.1 * total_steps))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2,  min_lr=1e-8\n",
    ")\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss / max(len(train_loader), 1)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            correct += (preds == batch[\"labels\"]).sum().item()\n",
    "            total += batch[\"labels\"].size(0)\n",
    "    avg_val_loss = val_loss / max(len(val_loader), 1)\n",
    "    scheduler.step(avg_val_loss)\n",
    "    val_acc = correct / max(total, 1)\n",
    "\n",
    "    # Track best for quick eyeballing\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: train_loss={avg_train_loss:.4f} val_loss={avg_val_loss:.4f} val_acc={val_acc:.3f} best_val_loss={best_val_loss:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lingu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
